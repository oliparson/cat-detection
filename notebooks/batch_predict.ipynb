{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import googleapiclient.discovery as discovery\n",
    "from googleapiclient import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bucket and blob prefix\n",
    "\n",
    "project = 'optimum-treat-262616'\n",
    "photo_bucket_name = 'catflap-photos-raw'\n",
    "model_bucket_name = 'cat-detection-models'\n",
    "prefix = '2020-06-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up buckets\n",
    "\n",
    "client = storage.Client()\n",
    "photo_bucket = client.get_bucket(photo_bucket_name)\n",
    "model_bucket = client.get_bucket(model_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5822\n"
     ]
    }
   ],
   "source": [
    "# Get list of blob names\n",
    "\n",
    "blobs = photo_bucket.list_blobs(prefix=prefix)\n",
    "blob_list = [blob.name for blob in blobs]\n",
    "print(len(blob_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-06-01_195953.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 22:18:03.256225 0\n",
      "2020-06-08 22:18:14.249526 100\n",
      "2020-06-08 22:18:24.887001 200\n",
      "2020-06-08 22:18:35.719818 300\n",
      "2020-06-08 22:18:46.465935 400\n",
      "2020-06-08 22:18:56.710459 500\n",
      "2020-06-08 22:19:06.812917 600\n",
      "2020-06-08 22:19:17.041849 700\n",
      "2020-06-08 22:19:27.906782 800\n",
      "2020-06-08 22:19:38.059869 900\n",
      "2020-06-08 22:19:48.982764 1000\n",
      "2020-06-08 22:19:59.772029 1100\n",
      "2020-06-08 22:20:10.196476 1200\n",
      "2020-06-08 22:20:20.370394 1300\n",
      "2020-06-08 22:20:30.579216 1400\n",
      "2020-06-08 22:20:41.030114 1500\n",
      "2020-06-08 22:20:51.259740 1600\n",
      "2020-06-08 22:21:01.741141 1700\n",
      "2020-06-08 22:21:11.698408 1800\n",
      "2020-06-08 22:21:22.245964 1900\n",
      "2020-06-08 22:21:32.177410 2000\n",
      "2020-06-08 22:21:42.416589 2100\n",
      "2020-06-08 22:21:52.588404 2200\n",
      "2020-06-08 22:22:02.680263 2300\n",
      "2020-06-08 22:22:12.593219 2400\n",
      "2020-06-08 22:22:22.860303 2500\n",
      "2020-06-08 22:22:32.941069 2600\n",
      "2020-06-08 22:22:43.170889 2700\n",
      "2020-06-08 22:22:53.303760 2800\n",
      "2020-06-08 22:23:03.470608 2900\n",
      "2020-06-08 22:23:13.311755 3000\n",
      "2020-06-08 22:23:23.794953 3100\n",
      "2020-06-08 22:23:34.551770 3200\n",
      "2020-06-08 22:23:44.778296 3300\n",
      "2020-06-08 22:23:55.177809 3400\n",
      "2020-06-08 22:24:05.378978 3500\n",
      "2020-06-08 22:24:15.412708 3600\n",
      "2020-06-08 22:24:25.527194 3700\n",
      "2020-06-08 22:24:35.777474 3800\n",
      "2020-06-08 22:24:46.644088 3900\n",
      "2020-06-08 22:24:57.214945 4000\n",
      "2020-06-08 22:25:07.403170 4100\n",
      "2020-06-08 22:25:17.632077 4200\n",
      "2020-06-08 22:25:27.639273 4300\n",
      "2020-06-08 22:25:38.180835 4400\n",
      "2020-06-08 22:25:48.569536 4500\n",
      "2020-06-08 22:25:58.817938 4600\n",
      "2020-06-08 22:26:09.240812 4700\n",
      "2020-06-08 22:26:19.256814 4800\n",
      "2020-06-08 22:26:29.229613 4900\n",
      "2020-06-08 22:26:39.410962 5000\n",
      "2020-06-08 22:26:49.690579 5100\n",
      "2020-06-08 22:27:00.609276 5200\n",
      "2020-06-08 22:27:10.948610 5300\n",
      "2020-06-08 22:27:21.127021 5400\n",
      "2020-06-08 22:27:31.714260 5500\n",
      "2020-06-08 22:27:41.783919 5600\n",
      "2020-06-08 22:27:52.054890 5700\n",
      "2020-06-08 22:28:02.501901 5800\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7faf5abbea70>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aedb356b53de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mblob_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_as_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mimg_red_downsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cat-detection-DpT3z93U/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             raise ValueError('Only know how to handle PNG; with Pillow '\n\u001b[1;32m   1463\u001b[0m                              'installed, Matplotlib can handle more images')\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cat-detection-DpT3z93U/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2894\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2896\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m     )\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7faf5abbea70>"
     ]
    }
   ],
   "source": [
    "# Read labels into pandas dataframe\n",
    "\n",
    "batch_input_filename = f'/home/jupyter/batch-input-{prefix}.json'\n",
    "with open(batch_input_filename, 'w') as fp:\n",
    "    for idx, blob_name in enumerate(blob_list[:]):\n",
    "\n",
    "        # Read blob from GCS\n",
    "        blob = photo_bucket.blob(blob_name)\n",
    "        blob_str = blob.download_as_string()\n",
    "        bytes_io = io.BytesIO(blob_str)\n",
    "        img = mpimg.imread(bytes_io, format='jpg')\n",
    "        img_red_downsample = img[::10,::10,0]\n",
    "\n",
    "        # Write to file\n",
    "        json_instances_dict = {'flatten_input': img_red_downsample.tolist(), 'key': blob_name}\n",
    "        json.dump(json_instances_dict, fp)\n",
    "        fp.write('\\n')\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(pd.Timestamp.now(), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload batch input json file to GCS\n",
    "\n",
    "batch_input_blob = model_bucket.blob('batch-input-keys/'+prefix+'.json')\n",
    "batch_input_blob.upload_from_filename(batch_input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_job_body(project_name, input_paths, output_path,\n",
    "        model_name, region, data_format='JSON',\n",
    "        version_name=None, max_worker_count=None,\n",
    "        runtime_version=None):\n",
    "\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    if version_name:\n",
    "        version_id = '{}/versions/{}'.format(model_id, version_name)\n",
    "\n",
    "    # Make a jobName of the format \"model_name_batch_predict_YYYYMMDD_HHMMSS\"\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.gmtime())\n",
    "\n",
    "    # Make sure the project name is formatted correctly to work as the basis\n",
    "    # of a valid job name.\n",
    "    clean_project_name = re.sub(r'\\W+', '_', project_name)\n",
    "\n",
    "    job_id = '{}_{}_{}'.format(clean_project_name, model_name,\n",
    "                           timestamp)\n",
    "\n",
    "    # Start building the request dictionary with required information.\n",
    "    body = {'jobId': job_id,\n",
    "            'predictionInput': {\n",
    "                'dataFormat': data_format,\n",
    "                'inputPaths': input_paths,\n",
    "                'outputPath': output_path,\n",
    "                'region': region}}\n",
    "\n",
    "    # Use the version if present, the model (its default version) if not.\n",
    "    if version_name:\n",
    "        body['predictionInput']['versionName'] = version_id\n",
    "    else:\n",
    "        body['predictionInput']['modelName'] = model_id\n",
    "\n",
    "    # Only include a maximum number of workers or a runtime version if specified.\n",
    "    # Otherwise let the service use its defaults.\n",
    "    if max_worker_count:\n",
    "        body['predictionInput']['maxWorkerCount'] = max_worker_count\n",
    "\n",
    "    if runtime_version:\n",
    "        body['predictionInput']['runtimeVersion'] = runtime_version\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'optimum_treat_262616_logistic_regression_v2_20200608_223238',\n",
       " 'predictionInput': {'dataFormat': 'JSON',\n",
       "  'inputPaths': 'gs://cat-detection-models/batch-input-keys/2020-06-01.json',\n",
       "  'outputPath': 'gs://cat-detection-models/batch-output-keys/2020-06-01/',\n",
       "  'region': 'europe-west2',\n",
       "  'versionName': 'projects/optimum-treat-262616/models/logistic_regression_v2/versions/logistic_regression_v2',\n",
       "  'maxWorkerCount': 20}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batch job body\n",
    "\n",
    "batch_predict_body = make_batch_job_body(\n",
    "    project_name = project, \n",
    "    input_paths = f'gs://{model_bucket_name}/batch-input-keys/{prefix}.json', \n",
    "    output_path = f'gs://{model_bucket_name}/batch-output-keys/{prefix}/',\n",
    "    model_name = 'logistic_regression_v2', \n",
    "    region = 'europe-west2',\n",
    "    version_name='logistic_regression_v2', \n",
    "    max_worker_count=20)\n",
    "\n",
    "batch_predict_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job requested.\n",
      "state : QUEUED\n"
     ]
    }
   ],
   "source": [
    "# Submit batch prediction job\n",
    "\n",
    "project_id = 'projects/{}'.format(project)\n",
    "\n",
    "ml = discovery.build('ml', 'v1')\n",
    "request = ml.projects().jobs().create(parent=project_id, body=batch_predict_body)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "\n",
    "    print('Job requested.')\n",
    "\n",
    "    # The state returned will almost always be QUEUED.\n",
    "    print('state : {}'.format(response['state']))\n",
    "\n",
    "except errors.HttpError as err:\n",
    "    # Something went wrong, print out some information.\n",
    "    print('There was an error getting the prediction results.' +\n",
    "          'Check the details:')\n",
    "    print(err._get_reason())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "cat-detection-4",
   "language": "python",
   "name": "cat-detection-4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
