{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import googleapiclient.discovery as discovery\n",
    "from googleapiclient import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bucket and blob prefix\n",
    "\n",
    "project = 'optimum-treat-262616'\n",
    "photo_bucket_name = 'catflap-photos-raw'\n",
    "model_bucket_name = 'cat-detection-models'\n",
    "prefix = '2020-06-07'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up buckets\n",
    "\n",
    "client = storage.Client()\n",
    "photo_bucket = client.get_bucket(photo_bucket_name)\n",
    "model_bucket = client.get_bucket(model_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3056\n"
     ]
    }
   ],
   "source": [
    "# Get list of blob names\n",
    "\n",
    "blobs = photo_bucket.list_blobs(prefix=prefix)\n",
    "blob_list = [blob.name for blob in blobs]\n",
    "print(len(blob_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 20:55:03.678352 0\n",
      "2020-06-09 20:55:15.888998 100\n",
      "2020-06-09 20:55:27.641290 200\n",
      "2020-06-09 20:55:40.221318 300\n",
      "2020-06-09 20:55:52.608006 400\n",
      "2020-06-09 20:56:04.347015 500\n",
      "2020-06-09 20:56:16.099585 600\n",
      "2020-06-09 20:56:27.871750 700\n",
      "2020-06-09 20:56:39.236348 800\n",
      "2020-06-09 20:56:50.389495 900\n",
      "2020-06-09 20:57:02.570344 1000\n",
      "2020-06-09 20:57:14.500155 1100\n",
      "2020-06-09 20:57:26.351837 1200\n",
      "2020-06-09 20:57:38.547688 1300\n",
      "2020-06-09 20:57:50.421774 1400\n",
      "2020-06-09 20:58:01.784858 1500\n",
      "2020-06-09 20:58:13.583922 1600\n",
      "2020-06-09 20:58:25.073078 1700\n",
      "2020-06-09 20:58:36.785304 1800\n",
      "2020-06-09 20:58:48.776435 1900\n",
      "2020-06-09 20:59:00.307508 2000\n",
      "2020-06-09 20:59:13.150699 2100\n",
      "2020-06-09 20:59:25.511384 2200\n",
      "2020-06-09 20:59:37.998924 2300\n",
      "2020-06-09 20:59:49.472976 2400\n",
      "2020-06-09 21:00:00.844571 2500\n",
      "2020-06-09 21:00:12.510390 2600\n",
      "2020-06-09 21:00:23.996578 2700\n",
      "2020-06-09 21:00:35.567576 2800\n",
      "2020-06-09 21:00:47.081896 2900\n",
      "2020-06-09 21:00:58.374019 3000\n"
     ]
    }
   ],
   "source": [
    "# Read labels into pandas dataframe\n",
    "\n",
    "batch_input_filename = f'/home/jupyter/batch-input-{prefix}.json'\n",
    "with open(batch_input_filename, 'w') as fp:\n",
    "    for idx, blob_name in enumerate(blob_list[:]):\n",
    "\n",
    "        # Read blob from GCS\n",
    "        blob = photo_bucket.blob(blob_name)\n",
    "        blob_str = blob.download_as_string()\n",
    "        bytes_io = io.BytesIO(blob_str)\n",
    "        img = mpimg.imread(bytes_io, format='jpg')\n",
    "        img_red_downsample = img[::10,::10,0]\n",
    "\n",
    "        # Write to file\n",
    "        json_instances_dict = {'flatten_input': img_red_downsample.tolist(), 'key': blob_name}\n",
    "        json.dump(json_instances_dict, fp)\n",
    "        fp.write('\\n')\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(pd.Timestamp.now(), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload batch input json file to GCS\n",
    "\n",
    "batch_input_blob = model_bucket.blob('batch-input-keys/'+prefix+'.json')\n",
    "batch_input_blob.upload_from_filename(batch_input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_job_body(project_name, input_paths, output_path,\n",
    "        model_name, region, data_format='JSON',\n",
    "        version_name=None, max_worker_count=None,\n",
    "        runtime_version=None):\n",
    "\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    if version_name:\n",
    "        version_id = '{}/versions/{}'.format(model_id, version_name)\n",
    "\n",
    "    # Make a jobName of the format \"model_name_batch_predict_YYYYMMDD_HHMMSS\"\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.gmtime())\n",
    "\n",
    "    # Make sure the project name is formatted correctly to work as the basis\n",
    "    # of a valid job name.\n",
    "    clean_project_name = re.sub(r'\\W+', '_', project_name)\n",
    "\n",
    "    job_id = '{}_{}_{}'.format(clean_project_name, model_name,\n",
    "                           timestamp)\n",
    "\n",
    "    # Start building the request dictionary with required information.\n",
    "    body = {'jobId': job_id,\n",
    "            'predictionInput': {\n",
    "                'dataFormat': data_format,\n",
    "                'inputPaths': input_paths,\n",
    "                'outputPath': output_path,\n",
    "                'region': region}}\n",
    "\n",
    "    # Use the version if present, the model (its default version) if not.\n",
    "    if version_name:\n",
    "        body['predictionInput']['versionName'] = version_id\n",
    "    else:\n",
    "        body['predictionInput']['modelName'] = model_id\n",
    "\n",
    "    # Only include a maximum number of workers or a runtime version if specified.\n",
    "    # Otherwise let the service use its defaults.\n",
    "    if max_worker_count:\n",
    "        body['predictionInput']['maxWorkerCount'] = max_worker_count\n",
    "\n",
    "    if runtime_version:\n",
    "        body['predictionInput']['runtimeVersion'] = runtime_version\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'optimum_treat_262616_logistic_regression_v2_20200609_210105',\n",
       " 'predictionInput': {'dataFormat': 'JSON',\n",
       "  'inputPaths': 'gs://cat-detection-models/batch-input-keys/2020-06-07.json',\n",
       "  'outputPath': 'gs://cat-detection-models/batch-output-keys/2020-06-07/',\n",
       "  'region': 'europe-west2',\n",
       "  'versionName': 'projects/optimum-treat-262616/models/logistic_regression_v2/versions/logistic_regression_v2',\n",
       "  'maxWorkerCount': 20}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batch job body\n",
    "\n",
    "batch_predict_body = make_batch_job_body(\n",
    "    project_name = project, \n",
    "    input_paths = f'gs://{model_bucket_name}/batch-input-keys/{prefix}.json', \n",
    "    output_path = f'gs://{model_bucket_name}/batch-output-keys/{prefix}/',\n",
    "    model_name = 'logistic_regression_v2', \n",
    "    region = 'europe-west2',\n",
    "    version_name='logistic_regression_v2', \n",
    "    max_worker_count=20)\n",
    "\n",
    "batch_predict_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job requested.\n",
      "state : QUEUED\n"
     ]
    }
   ],
   "source": [
    "# Submit batch prediction job\n",
    "\n",
    "project_id = 'projects/{}'.format(project)\n",
    "\n",
    "ml = discovery.build('ml', 'v1')\n",
    "request = ml.projects().jobs().create(parent=project_id, body=batch_predict_body)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "\n",
    "    print('Job requested.')\n",
    "\n",
    "    # The state returned will almost always be QUEUED.\n",
    "    print('state : {}'.format(response['state']))\n",
    "\n",
    "except errors.HttpError as err:\n",
    "    # Something went wrong, print out some information.\n",
    "    print('There was an error getting the prediction results.' +\n",
    "          'Check the details:')\n",
    "    print(err._get_reason())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "cat-detection-4",
   "language": "python",
   "name": "cat-detection-4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
